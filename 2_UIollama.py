# import streamlit as st    

# st.title("wow")

# st.write("hello world")

# text_input = st.sidebar.input_text("Enter text here")




# import streamlit as st

# st.title("ğŸ¦œğŸ”— Quickstart App")

# with st.form("my_form"):
#     text = st.text_area(
#         "Enter text:",
#         "What are the three key pieces of advice for learning how to code?",
#     )
#     submitted = st.form_submit_button("Submit")
#     # if not openai_api_key.startswith("sk-"):
#     #     st.warning("Please enter your OpenAI API key!", icon="âš ")
#     # if submitted and openai_api_key.startswith("sk-"):
#         #generate_response(text)
#     st.write(text)



# # ollama pull gemma2:9b #or 27b
# import streamlit as st 
# from langchain_community.llms import Ollama
# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
# from langchain.callbacks.manager import CallbackManager

# st.title("Gemma")

# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])


# def get_response(user_input):
#     # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ú©Ø§Ù„â€ŒØ¨Ú© Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ…ÛŒÙ†Ú¯
#     callback = StreamingCallback(response_placeholder)
#     callback_manager = CallbackManager([callback])

#     # Ø§Ø±Ø³Ø§Ù„ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„
#     llm.invoke(user_input, callbacks=callback_manager)


# from langchain_community.llms import Ollama
# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
# from langchain.callbacks.manager import CallbackManager

# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

# llm = Ollama(model="gemma2:9b", callbacks=callback_manager)

# def chat_with_llm():
#   """Handles the chat interaction with the LLM."""

#   while True:
#     user_input = st.text_area('Enter text here')
#     # submitted = st.form_submit_button("Submit")
#     if user_input == "exit":
#       print("Chatbot: Goodbye!")
#       break
#     if submitted :
#         response = llm.invoke(user_input)
#         print("Chatbot:", response)
#         st.write(response)

# # Start the chat loop
# chat_with_llm()




# import streamlit as st
# from langchain_community.llms import Ollama
# from langchain.callbacks.manager import CallbackManager
# from langchain.callbacks.base import BaseCallbackHandler

# # Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ…ÛŒÙ†Ú¯ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„
# class StreamCallbackHandler(BaseCallbackHandler):
#     def __init__(self, placeholder):
#         self.placeholder = placeholder
#         self.output = ""

#     def on_llm_new_token(self, token: str, **kwargs):
#         self.output += token
#         self.placeholder.markdown(self.output)  # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± Ø§Ø³ØªØ±ÛŒÙ…

# # Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ
# st.set_page_config(page_title="Gemma Chat", layout="wide")

# # Ø§ÛŒØ¬Ø§Ø¯ session Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
# callback_placeholder = st.empty()
# callback_manager = CallbackManager([])
# llm = Ollama(model="gemma2:9b", callbacks=callback_manager)

# # Ø±Ø§Ø¨Ø· Ú†Øª
# st.title("Gemma Chat")

# # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ Ùˆ Ø¨Ø¯Ù„â€ŒØ´Ø¯Ù‡
# for message in st.session_state.messages:
#     role = "User" if message["is_user"] else "Gemma"
#     st.markdown(f"**{role}:** {message['content']}")

# # ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
# with st.container():
#     user_input = st.text_input("Type your message:", key="user_input", placeholder="Enter your message here...")

#     if st.button("Send"):
#         if user_input.strip():
#             # Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
#             st.session_state.messages.append({"content": user_input, "is_user": True})
            
#             # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³ØªØ±ÛŒÙ…ÛŒÙ†Ú¯
#             placeholder = st.empty()
#             stream_handler = StreamCallbackHandler(placeholder)
#             callback_manager = CallbackManager([stream_handler])

#             # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø¯Ù„
#             try:
#                 llm = Ollama(model="gemma2:9b", callbacks=callback_manager)
#                 llm.invoke(user_input)
                
#                 # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
#                 st.session_state.messages.append({"content": stream_handler.output, "is_user": False})
#             except Exception as e:
#                 st.error(f"An error occurred: {e}")

#         # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ
#         st.session_state["user_input"] = ""































# import streamlit as st
# from langchain_community.llms import Ollama
# from langchain.callbacks.manager import CallbackManager
# from langchain.callbacks.base import BaseCallbackHandler

# # Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ…ÛŒÙ†Ú¯ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„
# class StreamCallbackHandler(BaseCallbackHandler):
#     def __init__(self, placeholder):
#         self.placeholder = placeholder
#         self.output = ""

#     def on_llm_new_token(self, token: str, **kwargs):
#         self.output += token
#         self.placeholder.markdown(self.output)  # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± Ø§Ø³ØªØ±ÛŒÙ…

# # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµÙØ­Ù‡
# st.set_page_config(page_title="Gemma Chat", layout="wide")

# # Ø§ÛŒØ¬Ø§Ø¯ session Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
# callback_manager = CallbackManager([])
# llm = Ollama(model="gemma2:9b", callbacks=callback_manager)

# # Ø¹Ù†ÙˆØ§Ù† ØµÙØ­Ù‡
# st.title("Gemma Chat")

# # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ Ùˆ Ø¨Ø¯Ù„â€ŒØ´Ø¯Ù‡
# chat_container = st.container()
# with chat_container:
#     for message in st.session_state.messages:
#         if message["is_user"]:
#             st.markdown(
#                 f"<div style='text-align: right; margin: 5px; padding: 10px; background-color: #DCF8C6; border-radius: 10px; display: inline-block; max-width: 80%;'>{message['content']}</div>",
#                 unsafe_allow_html=True,
#             )
#         else:
#             st.markdown(
#                 f"<div style='text-align: left; margin: 5px; padding: 10px; background-color: #FFFFFF; border-radius: 10px; display: inline-block; max-width: 80%;'>{message['content']}</div>",
#                 unsafe_allow_html=True,
#             )

# # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙØ§ØµÙ„Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø± Ù¾Ø§ÛŒÛŒÙ†
# st.write("\n" * 10)

# # ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
# user_input = st.text_input("Type your message:", key="user_input", placeholder="Enter your message here...")

# if st.button("Send"):
#     if user_input.strip():
#         # Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
#         st.session_state.messages.append({"content": user_input, "is_user": True})
        
#         # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³ØªØ±ÛŒÙ…ÛŒÙ†Ú¯
#         placeholder = st.empty()
#         stream_handler = StreamCallbackHandler(placeholder)
#         callback_manager = CallbackManager([stream_handler])

#         # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø¯Ù„
#         try:
#             llm = Ollama(model="gemma2:9b", callbacks=callback_manager)
#             llm.invoke(user_input)
            
#             # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
#             st.session_state.messages.append({"content": stream_handler.output, "is_user": False})
#         except Exception as e:
#             st.error(f"An error occurred: {e}")
        
#     # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ…
#     st.session_state.user_input = ""















































# import streamlit as st
# from langchain_community.llms import Ollama
# from langchain.callbacks.manager import CallbackManager
# from langchain.callbacks.base import BaseCallbackHandler

# # Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³ØªØ±ÛŒÙ…ÛŒÙ†Ú¯
# class StreamCallbackHandler(BaseCallbackHandler):
#     def __init__(self, placeholder):
#         self.placeholder = placeholder
#         self.output = ""

#     def on_llm_new_token(self, token: str, **kwargs):
#         self.output += token
#         self.placeholder.markdown(self.output)  # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø®Ø±ÙˆØ¬ÛŒ

# # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµÙØ­Ù‡
# st.set_page_config(page_title="Gemma Chat", layout="wide")

# # Ø§ÛŒØ¬Ø§Ø¯ session Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
# callback_manager = CallbackManager([])
# llm = Ollama(model="gemma2:9b", callbacks=callback_manager)

# # Ú†ÛŒØ¯Ù…Ø§Ù† Ú©Ù„ÛŒ ØµÙØ­Ù‡
# st.title("Gemma Chat")
# chat_container = st.container()

# # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
# with chat_container:
#     for message in st.session_state.messages:
#         if message["is_user"]:
#             st.markdown(
#                 f"""
#                 <div style='text-align: right; margin: 10px; padding: 10px; background-color: #DCF8C6; border-radius: 10px; max-width: 80%; display: inline-block;'>
#                     {message['content']}
#                 </div>
#                 """,
#                 unsafe_allow_html=True,
#             )
#         else:
#             st.markdown(
#                 f"""
#                 <div style='text-align: left; margin: 10px; padding: 10px; background-color: #F1F0F0; border-radius: 10px; max-width: 80%; display: inline-block;'>
#                     {message['content']}
#                 </div>
#                 """,
#                 unsafe_allow_html=True,
#             )

# # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ Ù¾Ø§ÛŒÛŒÙ† ØµÙØ­Ù‡
# st.write("<div style='position: fixed; bottom: 0; width: 100%; background-color: #FFFFFF; padding: 10px; box-shadow: 0px -2px 5px rgba(0,0,0,0.1);'>", unsafe_allow_html=True)

# # Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø®Ø´ ÙˆØ±ÙˆØ¯ÛŒ
# col1, col2 = st.columns([8, 1])
# with col1:
#     user_input = st.text_input(
#         "Type your message:",
#         placeholder="Enter your message here...",
#         key="user_input",
#         label_visibility="collapsed",
#     )
# with col2:
#     send_button = st.button("Send", key="send_button")

# # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
# if send_button and user_input.strip():
#     # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± Ø¨Ù‡ Ù„ÛŒØ³Øª
#     st.session_state.messages.append({"content": user_input, "is_user": True})

#     # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù… Ø¨Ù„Ø§ÙØ§ØµÙ„Ù‡ Ù¾Ø³ Ø§Ø² Ø§Ø±Ø³Ø§Ù„
#     with chat_container:
#         st.markdown(
#             f"""
#             <div style='text-align: right; margin: 10px; padding: 10px; background-color: #DCF8C6; border-radius: 10px; max-width: 80%; display: inline-block;'>
#                 {user_input}
#             </div>
#             """,
#             unsafe_allow_html=True,
#         )

#     # Ø§Ø³ØªØ±ÛŒÙ… Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
#     placeholder = st.empty()
#     stream_handler = StreamCallbackHandler(placeholder)
#     callback_manager = CallbackManager([stream_handler])

#     try:
#         llm = Ollama(model="gemma2:27b", callbacks=callback_manager)
#         llm.invoke(user_input)

#         # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
#         st.session_state.messages.append({"content": stream_handler.output, "is_user": False})
#     except Exception as e:
#         st.error(f"An error occurred: {e}")

#     # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ
#     st.experimental_rerun()






















































































import streamlit as st
from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler

# Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³ØªØ±ÛŒÙ…ÛŒÙ†Ú¯
class StreamCallbackHandler(BaseCallbackHandler):
    def __init__(self, placeholder):
        self.placeholder = placeholder
        self.output = ""

    def on_llm_new_token(self, token: str, **kwargs):
        self.output += token
        self.placeholder.markdown(self.output)  # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø®Ø±ÙˆØ¬ÛŒ

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø³ØªØ±ÛŒÙ…â€ŒÙ„ÛŒØª
st.set_page_config(page_title="Gemma Chat", layout="wide")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
callback_manager = CallbackManager([])
llm = Ollama(model="gemma2:27b", callbacks=callback_manager)

# Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¯Ø± session_state
if "messages" not in st.session_state:
    st.session_state.messages = []

# ØªØ§Ø¨Ø¹ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ø¯Ù„
def get_response(context, user_input, placeholder):
    # ØªØ±Ú©ÛŒØ¨ Ú©Ø§Ù†ØªÚ©Ø³Øª Ù…Ú©Ø§Ù„Ù…Ù‡
    full_input = "\n".join([f"You: {msg['content']}" if msg['is_user'] else f"Gemma: {msg['content']}" for msg in context])
    full_input += f"\nYou: {user_input}\nGemma:"
    
    # Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³ØªØ±ÛŒÙ…
    stream_handler = StreamCallbackHandler(placeholder)
    callback_manager = CallbackManager([stream_handler])
    
    # Ø§Ø±Ø³Ø§Ù„ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„
    llm.callbacks = callback_manager
    llm.invoke(full_input)

    return stream_handler.output

# Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
st.title("Gemma Chat")
chat_container = st.container()
with chat_container:
    for message in st.session_state.messages:
        if message["is_user"]:
            st.markdown(f"<div style='text-align: right; color: blue;'>{message['content']}</div>", unsafe_allow_html=True)
        else:
            st.markdown(f"<div style='text-align: left; color: green;'>{message['content']}</div>", unsafe_allow_html=True)

# ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
user_input = st.text_input("Type your message:", key="user_input")

if st.button("Send"):
    if user_input.strip():
        # Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
        st.session_state.messages.append({"content": user_input, "is_user": True})
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
        placeholder = st.empty()
        response = get_response(st.session_state.messages, user_input, placeholder)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
        st.session_state.messages.append({"content": response, "is_user": False})

        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ
        st.experimental_rerun()
